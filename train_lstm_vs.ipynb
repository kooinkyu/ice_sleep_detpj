{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca9455-a3c7-4bf8-a09b-1e89d2a40f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a87b0-cebe-4949-8f5f-7e37635fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ice_sleep_detpj/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca983a6c-4f2b-40dd-980a-c513f1734c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f08255-80a8-4e0c-9201-3fa436f28eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q /home/jovyan/ice_sleep_detpj/data/ice_control_dataset/ice_project/Validation/[원천]keypoint_준통제환경.zip -d /home/jovyan/ice_sleep_detpj/data/ice_control_dataset/ice_project/Validation/[원천]keypoint_준통제환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42f169-d1a0-4966-8a40-a6227f305117",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d952d7-97cb-415a-ad60-9eefafb68d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/jovyan/ice_sleep_detpj/data/ice_lstm_seq_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b74968-aa78-469a-945a-3ff0c029c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_bbox(bbox, img_width, img_height):\n",
    "    x_min, y_min, x_max, y_max = map(float, bbox)\n",
    "    x_center = (x_min + x_max) / 2 / img_width\n",
    "    y_center = (y_min + y_max) / 2 / img_height\n",
    "    w = (x_max - x_min) / img_width\n",
    "    h = (y_max - y_min) / img_height\n",
    "    return x_center, y_center, w, h\n",
    "\n",
    "def merge_bboxes(b1, b2):\n",
    "    x_min = min(float(b1[0]), float(b2[0]))\n",
    "    y_min = min(float(b1[1]), float(b2[1]))\n",
    "    x_max = max(float(b1[2]), float(b2[2]))\n",
    "    y_max = max(float(b1[3]), float(b2[3]))\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "def json_to_yolo(json_path, output_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    width = int(data['FileInfo']['Width'])\n",
    "    height = int(data['FileInfo']['Height'])\n",
    "    bboxes = data['ObjectInfo']['BoundingBox']\n",
    "\n",
    "    result_lines = []\n",
    "\n",
    "    # 👁️ 눈 (양쪽 눈 기준 합쳐서)\n",
    "    leye = bboxes.get('Leye')\n",
    "    reye = bboxes.get('Reye')\n",
    "    if leye['isVisible'] and reye['isVisible']:\n",
    "        is_open = leye['Opened'] or reye['Opened']\n",
    "        class_id = 0 if is_open else 1  # 0: eye_open, 1: eye_closed\n",
    "        merged_box = merge_bboxes(leye['Position'], reye['Position'])\n",
    "        coords = convert_bbox(merged_box, width, height)\n",
    "        result_lines.append(f\"{class_id} {' '.join(map(str, coords))}\")\n",
    "\n",
    "    # 👄 입\n",
    "    mouth = bboxes.get('Mouth')\n",
    "    if mouth['isVisible']:\n",
    "        class_id = 2 if mouth['Opened'] else 3  # 2: mouth_open, 3: mouth_closed\n",
    "        coords = convert_bbox(mouth['Position'], width, height)\n",
    "        result_lines.append(f\"{class_id} {' '.join(map(str, coords))}\")\n",
    "\n",
    "    # 저장\n",
    "    base_name = os.path.splitext(os.path.basename(json_path))[0]\n",
    "    output_file = os.path.join(output_path, base_name + '.txt')\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(result_lines))\n",
    "\n",
    "# 📁 Jupyter 환경 기준 경로 (수정됨)\n",
    "input_dir = 'data/ice_th/YOLO_dataset/labels/val'        # JSON 파일들\n",
    "output_dir = 'data/ice_th/YOLO_dataset/labels/val_txt'   # YOLO txt 저장 경로\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 변환 실행\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(input_dir, filename)\n",
    "        json_to_yolo(json_path, output_dir)\n",
    "\n",
    "print(\"✅ 변환 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16bcf0-b9c9-41c0-921f-7e14de70f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def generate_y_true_from_folders(controlled_root, realroad_root, save_path):\n",
    "    \"\"\"\n",
    "    통제환경은 졸음(1), 실제 도로 환경은 정상(0)으로 라벨링.\n",
    "    폴더명(사람 단위)을 기준으로 라벨 생성 후 numpy 배열로 저장.\n",
    "\n",
    "    Args:\n",
    "        controlled_root (str): 통제환경 데이터 루트 폴더 경로\n",
    "        realroad_root (str): 실제 도로 환경 데이터 루트 폴더 경로\n",
    "        save_path (str): 저장할 y_true 경로 (npy 파일)\n",
    "\n",
    "    Returns:\n",
    "        y_true (np.ndarray): 라벨 배열 (0: 정상, 1: 졸음)\n",
    "        person_ids (List[str]): 사람 ID 리스트 (폴더명 기준)\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    person_ids = []\n",
    "\n",
    "    # ✅ 통제환경: 졸음 = 1\n",
    "    if os.path.exists(controlled_root):\n",
    "        for person_folder in sorted(os.listdir(controlled_root)):\n",
    "            full_path = os.path.join(controlled_root, person_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                person_ids.append(person_folder)\n",
    "                y_true.append(1)\n",
    "\n",
    "    # ✅ 실제 도로 환경: 정상 = 0\n",
    "    if os.path.exists(realroad_root):\n",
    "        for person_folder in sorted(os.listdir(realroad_root)):\n",
    "            full_path = os.path.join(realroad_root, person_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                person_ids.append(person_folder)\n",
    "                y_true.append(0)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    np.save(save_path, y_true)\n",
    "\n",
    "    print(f\"\\n✅ y_true 저장 완료: {save_path}\")\n",
    "    print(f\"   총 인원: {len(y_true)}명 | 졸음: {np.sum(y_true)}, 정상: {len(y_true) - np.sum(y_true)}\")\n",
    "    return y_true, person_ids\n",
    "\n",
    "\n",
    "# ✅ Train 데이터 라벨링\n",
    "controlled_root = \"data/ice_control_dataset/ice_project/Training/[라벨]bbox_통제환경\"\n",
    "realroad_root = \"data/ice_control_dataset/ice_project/Training/[라벨]bbox_실제도로환경/2.승용\"\n",
    "y_true_train, person_ids_train = generate_y_true_from_folders(\n",
    "    controlled_root,\n",
    "    realroad_root,\n",
    "    save_path=\"data/y_true_labeling_train.npy\"\n",
    ")\n",
    "\n",
    "# ✅ Validation 데이터 라벨링\n",
    "controlled_val_root = \"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_통제환경\"\n",
    "realroad_val_root = \"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_실제도로환경/2.승용\"\n",
    "y_true_val, person_ids_val = generate_y_true_from_folders(\n",
    "    controlled_val_root,\n",
    "    realroad_val_root,\n",
    "    save_path=\"data/y_true_labeling_val.npy\"\n",
    ")\n",
    "\n",
    "# ✅ 전체 person_id와 y_true 합치기 (통합 label_dict 만들기 원할 경우)\n",
    "all_person_ids = person_ids_train + person_ids_val\n",
    "all_y_true = np.concatenate([y_true_train, y_true_val])\n",
    "\n",
    "label_dict = dict(zip(all_person_ids, all_y_true))\n",
    "print(f\"\\n✅ 통합 label_dict 생성 완료! 총 인원 수: {len(label_dict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e2cef-0d0f-4c90-83e4-0da6c9d1be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Feature + person_id 추출 함수\n",
    "def extract_feature_with_id(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    bb = data[\"ObjectInfo\"][\"BoundingBox\"]\n",
    "\n",
    "    def safe_area(pos):\n",
    "        try:\n",
    "            return max(0, (float(pos[2]) - float(pos[0])) * (float(pos[3]) - float(pos[1])))\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    left_eye_open = int(bb[\"Leye\"][\"Opened\"])\n",
    "    right_eye_open = int(bb[\"Reye\"][\"Opened\"])\n",
    "    mouth_open = int(bb[\"Mouth\"][\"Opened\"])\n",
    "\n",
    "    face_area = safe_area(bb[\"Face\"][\"Position\"])\n",
    "    left_eye_area = safe_area(bb[\"Leye\"][\"Position\"])\n",
    "    right_eye_area = safe_area(bb[\"Reye\"][\"Position\"])\n",
    "    mouth_area = safe_area(bb[\"Mouth\"][\"Position\"])\n",
    "\n",
    "    if face_area > 0:\n",
    "        left_eye_ratio = left_eye_area / face_area\n",
    "        right_eye_ratio = right_eye_area / face_area\n",
    "        mouth_ratio = mouth_area / face_area\n",
    "        face_area_log = np.log(face_area + 1)\n",
    "    else:\n",
    "        left_eye_ratio = right_eye_ratio = mouth_ratio = face_area_log = 0.0\n",
    "\n",
    "    feature = [\n",
    "        left_eye_open,\n",
    "        right_eye_open,\n",
    "        mouth_open,\n",
    "        face_area_log,\n",
    "        left_eye_ratio,\n",
    "        right_eye_ratio,\n",
    "        mouth_ratio,\n",
    "    ]\n",
    "\n",
    "    person_id = os.path.basename(os.path.dirname(json_path))\n",
    "    return feature, person_id\n",
    "\n",
    "# ✅ 시퀀스 생성 함수\n",
    "def create_sequences_with_id(json_dir, label_dict, save_dir, seq_len=30):\n",
    "    X_seq, y_true_seq = [], []\n",
    "    person_to_features = {}\n",
    "\n",
    "    for root, _, files in os.walk(json_dir):\n",
    "        for jf in sorted(files):\n",
    "            if jf.endswith('.json'):\n",
    "                try:\n",
    "                    fpath = os.path.join(root, jf)\n",
    "                    feat, pid = extract_feature_with_id(fpath)\n",
    "                    person_to_features.setdefault(pid, []).append(feat)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ 오류: {jf} - {e}\")\n",
    "\n",
    "    for pid, features in person_to_features.items():\n",
    "        for i in range(len(features) - seq_len):\n",
    "            seq = features[i:i+seq_len]\n",
    "            X_seq.append(seq)\n",
    "            y_true_seq.append(label_dict.get(pid, 0))\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_true_seq = np.array(y_true_seq)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, \"X_seq.npy\"), X_seq)\n",
    "    np.save(os.path.join(save_dir, \"y_true_seq.npy\"), y_true_seq)\n",
    "\n",
    "    print(f\"\\n✅ 저장 완료: {save_dir}\")\n",
    "    print(f\"  시퀀스 수: {len(X_seq)} | 졸음 비율: {np.mean(y_true_seq):.2f}\")\n",
    "    return X_seq, y_true_seq\n",
    "\n",
    "# ✅ person_ids와 y_true 저장 로직\n",
    "train_person_ids, val_person_ids = [], []\n",
    "y_true_train, y_true_val = [], []\n",
    "\n",
    "# 🔹 Train\n",
    "train_paths = [\n",
    "    (\"data/ice_control_dataset/ice_project/Training/[라벨]bbox_통제환경\", 1),\n",
    "    (\"data/ice_control_dataset/ice_project/Training/[라벨]bbox_실제도로환경/2.승용\", 0)\n",
    "]\n",
    "for folder, label in train_paths:\n",
    "    for person_folder in sorted(os.listdir(folder)):\n",
    "        if os.path.isdir(os.path.join(folder, person_folder)):\n",
    "            train_person_ids.append(person_folder)\n",
    "            y_true_train.append(label)\n",
    "\n",
    "# 🔹 Validation\n",
    "val_paths = [\n",
    "    (\"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_통제환경\", 1),\n",
    "    (\"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_실제도로환경/2.승용\", 0)\n",
    "]\n",
    "for folder, label in val_paths:\n",
    "    for person_folder in sorted(os.listdir(folder)):\n",
    "        if os.path.isdir(os.path.join(folder, person_folder)):\n",
    "            val_person_ids.append(person_folder)\n",
    "            y_true_val.append(label)\n",
    "\n",
    "# ✅ 저장\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "np.save(\"data/y_true_labeling_train.npy\", np.array(y_true_train))\n",
    "np.save(\"data/y_true_labeling_val.npy\", np.array(y_true_val))\n",
    "with open(\"data/person_ids_train.txt\", \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(train_person_ids))\n",
    "with open(\"data/person_ids_val.txt\", \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(val_person_ids))\n",
    "\n",
    "# ✅ label_dict 구성 (train + val 전체 기준)\n",
    "all_person_ids = train_person_ids + val_person_ids\n",
    "all_y_true = np.concatenate([y_true_train, y_true_val])\n",
    "label_dict = dict(zip(all_person_ids, all_y_true))\n",
    "print(f\"\\n📌 label_dict 총 인원: {len(label_dict)}\")\n",
    "\n",
    "# ✅ 시퀀스 생성\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Training/[라벨]bbox_통제환경\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_통제_ytrue/train\"\n",
    ")\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_통제환경\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_통제_ytrue/val\"\n",
    ")\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Training/[라벨]bbox_실제도로환경/2.승용\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_실제도로환경_ytrue/train\"\n",
    ")\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_실제도로환경/2.승용\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_실제도로환경_ytrue/val\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160999c4-7d7c-4b89-9885-14c5634cadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ✅ 실제 도로 환경 (승용)\n",
    "X_train_s = np.load(\"data/lstm_seq_실제도로환경_ytrue/train/X_seq.npy\")\n",
    "y_train_s = np.load(\"data/lstm_seq_실제도로환경_ytrue/train/y_true_seq.npy\")\n",
    "X_val_s = np.load(\"data/lstm_seq_실제도로환경_ytrue/val/X_seq.npy\")\n",
    "y_val_s = np.load(\"data/lstm_seq_실제도로환경_ytrue/val/y_true_seq.npy\")\n",
    "\n",
    "# ✅ 통제 환경\n",
    "X_train_c = np.load(\"data/lstm_seq_통제_ytrue/train/X_seq.npy\")\n",
    "y_train_c = np.load(\"data/lstm_seq_통제_ytrue/train/y_true_seq.npy\")\n",
    "X_val_c = np.load(\"data/lstm_seq_통제_ytrue/val/X_seq.npy\")\n",
    "y_val_c = np.load(\"data/lstm_seq_통제_ytrue/val/y_true_seq.npy\")\n",
    "\n",
    "# ✅ 통합\n",
    "X_train_combined = np.concatenate([X_train_s, X_train_c], axis=0)\n",
    "y_train_combined = np.concatenate([y_train_s, y_train_c], axis=0)\n",
    "X_val_combined = np.concatenate([X_val_s, X_val_c], axis=0)\n",
    "y_val_combined = np.concatenate([y_val_s, y_val_c], axis=0)\n",
    "\n",
    "# ✅ 저장 디렉토리\n",
    "save_dir = \"data/ice_lstm_seq_combined_ytrue\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ✅ 저장\n",
    "np.save(os.path.join(save_dir, \"X_seq.npy\"), X_train_combined)\n",
    "np.save(os.path.join(save_dir, \"y_seq.npy\"), y_train_combined)\n",
    "np.save(os.path.join(save_dir, \"X_val_seq.npy\"), X_val_combined)\n",
    "np.save(os.path.join(save_dir, \"y_val_seq.npy\"), y_val_combined)\n",
    "\n",
    "# ✅ 요약 출력\n",
    "print(\"\\n✅ y_true 기반 시퀀스 통합 저장 완료!\")\n",
    "print(f\"🔹 Train: X = {X_train_combined.shape}, y = {y_train_combined.shape} (졸음: {np.sum(y_train_combined)}, 정상: {len(y_train_combined)-np.sum(y_train_combined)})\")\n",
    "print(f\"🔹  Val : X = {X_val_combined.shape}, y = {y_val_combined.shape} (졸음: {np.sum(y_val_combined)}, 정상: {len(y_val_combined)-np.sum(y_val_combined)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b738db-c2ca-4a4e-b653-54a77aa2d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Feature + person_id + path 추출 함수\n",
    "def extract_feature_with_id(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    bb = data[\"ObjectInfo\"][\"BoundingBox\"]\n",
    "\n",
    "    def safe_area(pos):\n",
    "        try:\n",
    "            return max(0, (float(pos[2]) - float(pos[0])) * (float(pos[3]) - float(pos[1])) )\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    left_eye_open = int(bb[\"Leye\"][\"Opened\"])\n",
    "    right_eye_open = int(bb[\"Reye\"][\"Opened\"])\n",
    "    mouth_open = int(bb[\"Mouth\"][\"Opened\"])\n",
    "\n",
    "    face_area = safe_area(bb[\"Face\"][\"Position\"])\n",
    "    left_eye_area = safe_area(bb[\"Leye\"][\"Position\"])\n",
    "    right_eye_area = safe_area(bb[\"Reye\"][\"Position\"])\n",
    "    mouth_area = safe_area(bb[\"Mouth\"][\"Position\"])\n",
    "\n",
    "    if face_area > 0:\n",
    "        left_eye_ratio = left_eye_area / face_area\n",
    "        right_eye_ratio = right_eye_area / face_area\n",
    "        mouth_ratio = mouth_area / face_area\n",
    "        face_area_log = np.log(face_area + 1)\n",
    "    else:\n",
    "        left_eye_ratio = right_eye_ratio = mouth_ratio = face_area_log = 0.0\n",
    "\n",
    "    feature = [\n",
    "        left_eye_open,\n",
    "        right_eye_open,\n",
    "        mouth_open,\n",
    "        face_area_log,\n",
    "        left_eye_ratio,\n",
    "        right_eye_ratio,\n",
    "        mouth_ratio,\n",
    "    ]\n",
    "\n",
    "    person_id = os.path.basename(os.path.dirname(json_path))\n",
    "    return feature, person_id, json_path\n",
    "\n",
    "# ✅ 커스텀 라벨 기반 시퀀스 생성 함수\n",
    "def create_sequences_with_custom_label(json_dir, save_dir, seq_len=30):\n",
    "    X_seq, y_seq = [], []\n",
    "    person_to_features = {}\n",
    "\n",
    "    for root, _, files in os.walk(json_dir):\n",
    "        for jf in sorted(files):\n",
    "            if jf.endswith('.json'):\n",
    "                try:\n",
    "                    fpath = os.path.join(root, jf)\n",
    "                    feat, pid, full_path = extract_feature_with_id(fpath)\n",
    "                    person_to_features.setdefault(pid, []).append((feat, full_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ 오류: {jf} - {e}\")\n",
    "\n",
    "    for pid, feat_path_list in person_to_features.items():\n",
    "        features, paths = zip(*feat_path_list)\n",
    "\n",
    "        for i in range(len(features) - seq_len):\n",
    "            seq_feats = features[i:i+seq_len]\n",
    "            seq_paths = paths[i:i+seq_len]\n",
    "\n",
    "            open_stat = np.array(seq_feats)[:, :2]\n",
    "            both_closed = np.sum((open_stat[:, 0] == 0) & (open_stat[:, 1] == 0))\n",
    "\n",
    "            joined_path_str = \" \".join(seq_paths)\n",
    "            if \"졸음재현\" in joined_path_str or \"하품\" in joined_path_str or both_closed >= 3:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            X_seq.append(seq_feats)\n",
    "            y_seq.append(label)\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, \"X_seq.npy\"), X_seq)\n",
    "    np.save(os.path.join(save_dir, \"y_seq.npy\"), y_seq)\n",
    "\n",
    "    print(f\"\\n✅ 저장 완료: {save_dir}\")\n",
    "    print(f\"  🔹 시퀀스 수: {len(X_seq)}\")\n",
    "    print(f\"  👁 졸음 시퀀스 수: {np.sum(y_seq)} | 😀 정상 시퀀스 수: {len(y_seq) - np.sum(y_seq)}\")\n",
    "    return X_seq, y_seq\n",
    "\n",
    "# ✅ 시퀀스 생성: train + val\n",
    "X_train, y_train = create_sequences_with_custom_label(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Training/[라벨]bbox_통제환경\",\n",
    "    save_dir=\"data/lstm_seq_custom/train\"\n",
    ")\n",
    "\n",
    "X_val, y_val = create_sequences_with_custom_label(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_통제환경\",\n",
    "    save_dir=\"data/lstm_seq_custom/val\"\n",
    ")\n",
    "\n",
    "# ✅ 통합 저장\n",
    "save_dir = \"data/ice_lstm_seq_combined_custom\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, \"X_seq.npy\"), X_train)\n",
    "np.save(os.path.join(save_dir, \"y_seq.npy\"), y_train)\n",
    "np.save(os.path.join(save_dir, \"X_val_seq.npy\"), X_val)\n",
    "np.save(os.path.join(save_dir, \"y_val_seq.npy\"), y_val)\n",
    "\n",
    "print(\"\\n🎉 전체 통합 저장 완료!\")\n",
    "print(f\"🔹 Train: X = {X_train.shape}, y = {y_train.shape} (졸음: {np.sum(y_train)}, 정상: {len(y_train)-np.sum(y_train)})\")\n",
    "print(f\"🔹  Val : X = {X_val.shape}, y = {y_val.shape} (졸음: {np.sum(y_val)}, 정상: {len(y_val)-np.sum(y_val)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f9c19-f87b-42dd-9c4c-a7b34b1b44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# ✅ 1. 데이터 로드 (경로 수정)\n",
    "X_train = np.load(\"data/ice_lstm_seq_combined_custom/X_seq.npy\")\n",
    "y_train = np.load(\"data/ice_lstm_seq_combined_custom/y_seq.npy\")\n",
    "X_val = np.load(\"data/ice_lstm_seq_combined_custom/X_val_seq.npy\")\n",
    "y_val = np.load(\"data/ice_lstm_seq_combined_custom/y_val_seq.npy\")\n",
    "\n",
    "print(f\"\\n✅ 데이터 로드 완료:\")\n",
    "print(f\"🔹 X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"🔹 X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"📊 Train 클래스 분포: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "print(f\"📊 Val   클래스 분포: {dict(zip(*np.unique(y_val, return_counts=True)))}\")\n",
    "\n",
    "# ✅ 2. Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_2d).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val_2d).reshape(X_val.shape)\n",
    "\n",
    "joblib.dump(scaler, \"feature_scaler_custom.pkl\")\n",
    "print(\"✅ Feature Scaling 완료 및 저장됨: feature_scaler_custom.pkl\")\n",
    "\n",
    "# ✅ 3. 클래스 가중치 계산\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"✅ 클래스 가중치: {class_weight_dict}\")\n",
    "\n",
    "# ✅ 4. 모델 정의\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# ✅ 5. 콜백 정의\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\"best_model_lstm_custom.h5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "# ✅ 6. 학습\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ✅ 7. 결과 요약\n",
    "print(\"\\n🎉 학습 완료!\")\n",
    "print(f\"🔹 Train Loss: {min(history.history['loss']):.4f}\")\n",
    "print(f\"🔹 Val Loss:   {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"🔹 Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"🔹 Val Precision: {max(history.history['val_precision']):.4f}\")\n",
    "print(f\"🔹 Val Recall:    {max(history.history['val_recall']):.4f}\")\n",
    "\n",
    "# ✅ 8. 학습 곡선 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['precision'], label='Train Precision')\n",
    "plt.plot(history.history['val_precision'], label='Val Precision')\n",
    "plt.plot(history.history['recall'], label='Train Recall')\n",
    "plt.plot(history.history['val_recall'], label='Val Recall')\n",
    "plt.title('Precision/Recall Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705f92e-569c-47a3-86ae-4c92be0440bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e2734-e205-40dc-91de-000e45949061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클로드 코드 - 통제환경 + 실제도로 환경 결합 버전\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ real_road 라벨 디버그용 리스트\n",
    "realroad_debug_values = []\n",
    "\n",
    "def extract_feature_with_id(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    bb = data['ObjectInfo']['BoundingBox']\n",
    "\n",
    "    leye_open = int(bb['Leye']['Opened']) if 'Opened' in bb['Leye'] else 0\n",
    "    reye_open = int(bb['Reye']['Opened']) if 'Opened' in bb['Reye'] else 0\n",
    "    mouth_open = int(bb['Mouth']['Opened']) if 'Opened' in bb['Mouth'] else 0\n",
    "\n",
    "    def safe_area(pos):\n",
    "        try:\n",
    "            return max(0, (float(pos[2]) - float(pos[0])) * (float(pos[3]) - float(pos[1])))\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    face_area = safe_area(bb['Face']['Position'])\n",
    "    if face_area < 10:\n",
    "        face_area = 1e-6\n",
    "\n",
    "    left_eye_area = safe_area(bb['Leye']['Position'])\n",
    "    right_eye_area = safe_area(bb['Reye']['Position'])\n",
    "    mouth_area = safe_area(bb['Mouth']['Position'])\n",
    "\n",
    "    left_eye_ratio = left_eye_area / face_area\n",
    "    right_eye_ratio = right_eye_area / face_area\n",
    "    mouth_ratio = mouth_area / face_area\n",
    "    face_area_log = np.log(face_area + 1)\n",
    "\n",
    "    features = [\n",
    "        float(leye_open),\n",
    "        float(reye_open),\n",
    "        float(mouth_open),\n",
    "        float(face_area_log),\n",
    "        float(left_eye_ratio),\n",
    "        float(right_eye_ratio),\n",
    "        float(mouth_ratio)\n",
    "    ]\n",
    "\n",
    "    person_id = os.path.basename(json_path).split('_')[0]\n",
    "    return features, person_id, json_path\n",
    "\n",
    "\n",
    "# ✅ 시각화 함수\n",
    "def visualize_realroad_debug(values):\n",
    "    max_consecutives = [v['max_consecutive'] for v in values]\n",
    "    closed_ratios = [v['closed_ratio'] for v in values]\n",
    "    labels = [v['label'] for v in values]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([m for m, l in zip(max_consecutives, labels) if l == 1], bins=30, alpha=0.7, label='Drowsy')\n",
    "    plt.hist([m for m, l in zip(max_consecutives, labels) if l == 0], bins=30, alpha=0.7, label='Normal')\n",
    "    plt.title('Max Consecutive Eye Closures (real_road)')\n",
    "    plt.xlabel('max_consecutive')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([r for r, l in zip(closed_ratios, labels) if l == 1], bins=30, alpha=0.7, label='Drowsy')\n",
    "    plt.hist([r for r, l in zip(closed_ratios, labels) if l == 0], bins=30, alpha=0.7, label='Normal')\n",
    "    plt.title('Closed Ratio (real_road)')\n",
    "    plt.xlabel('closed_ratio')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_combined_sequences(data_paths, save_dir, seq_len=16):\n",
    "    \"\"\"\n",
    "    여러 환경의 데이터를 결합하여 시퀀스 생성\n",
    "    data_paths: dict형태로 {'environment_name': 'path'} \n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    person_to_features = {}\n",
    "\n",
    "    # 디버깅을 위한 카운터\n",
    "    label_debug = {\n",
    "        'drowsy_path': 0,\n",
    "        'drowsy_eyes': 0,\n",
    "        'normal': 0,\n",
    "        'by_environment': {}\n",
    "    }\n",
    "\n",
    "    print(f\"🔄 다음 환경들의 데이터를 결합합니다:\")\n",
    "    for env_name, env_path in data_paths.items():\n",
    "        print(f\"  📁 {env_name}: {env_path}\")\n",
    "        label_debug['by_environment'][env_name] = {'drowsy': 0, 'normal': 0}\n",
    "\n",
    "    # 각 환경별로 데이터 수집\n",
    "    for env_name, json_dir in data_paths.items():\n",
    "        print(f\"\\n🔍 {env_name} 환경 처리 중...\")\n",
    "        env_person_features = {}\n",
    "\n",
    "        if not os.path.exists(json_dir):\n",
    "            print(f\"⚠️ 경로가 존재하지 않습니다: {json_dir}\")\n",
    "            continue\n",
    "\n",
    "        file_count = 0\n",
    "        for root, _, files in os.walk(json_dir):\n",
    "            for jf in sorted(files):\n",
    "                if jf.endswith('.json'):\n",
    "                    try:\n",
    "                        fpath = os.path.join(root, jf)\n",
    "                        feat, pid, full_path = extract_feature_with_id(fpath)\n",
    "                        if feat is not None:\n",
    "                            combined_pid = f\"{env_name}_{pid}\"\n",
    "                            env_person_features.setdefault(combined_pid, []).append((feat, full_path, env_name))\n",
    "                            file_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ 오류: {jf} - {e}\")\n",
    "\n",
    "        print(f\"  ✅ {file_count}개 파일 처리 완료\")\n",
    "\n",
    "        for pid, feat_list in env_person_features.items():\n",
    "            person_to_features[pid] = feat_list\n",
    "\n",
    "    print(f\"\\n📊 전체 수집된 사람 수: {len(person_to_features)}\")\n",
    "\n",
    "    # 시퀀스 생성\n",
    "    for pid, feat_path_env_list in person_to_features.items():\n",
    "        features, paths, envs = zip(*feat_path_env_list)\n",
    "\n",
    "        if len(features) < seq_len:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(features) - seq_len):\n",
    "            seq_feats = features[i:i+seq_len]\n",
    "            seq_paths = paths[i:i+seq_len]\n",
    "            seq_env = envs[i]  # 첫 번째 프레임의 환경\n",
    "\n",
    "            # ✅ 유효한 환경인지 확인\n",
    "            assert seq_env in label_debug['by_environment'], f\"❌ Unknown environment type: {seq_env}\"\n",
    "\n",
    "            open_stat = np.array(seq_feats)[:, :2]\n",
    "            both_closed = (open_stat[:, 0] == 0) & (open_stat[:, 1] == 0)\n",
    "\n",
    "            consecutive_closed = 0\n",
    "            max_consecutive = 0\n",
    "            for closed in both_closed:\n",
    "                if closed:\n",
    "                    consecutive_closed += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive_closed)\n",
    "                else:\n",
    "                    consecutive_closed = 0\n",
    "\n",
    "            closed_ratio = np.sum(both_closed) / len(both_closed)\n",
    "\n",
    "            joined_path_str = \" \".join(seq_paths)\n",
    "            drowsy_keywords = [\"졸음재현\", \"하품\", \"졸음\", \"drowsy\", \"yawn\", \"sleepy\", \"tired\"]\n",
    "            normal_keywords = [\"정상\", \"normal\", \"alert\", \"awake\"]\n",
    "\n",
    "            path_indicates_drowsy = any(keyword in joined_path_str.lower() for keyword in drowsy_keywords)\n",
    "            path_indicates_normal = any(keyword in joined_path_str.lower() for keyword in normal_keywords)\n",
    "            \n",
    "\n",
    "\n",
    "            if seq_env == \"controlled\":\n",
    "                is_drowsy = (max_consecutive >= 4 or closed_ratio >= 0.35)\n",
    "                if not is_drowsy and path_indicates_drowsy:\n",
    "                    is_drowsy = True\n",
    "                if max_consecutive < 2 and closed_ratio < 0.15: #이거 manx_consecutive 2로 바꿔서 하 생각.\n",
    "                    is_drowsy = False\n",
    "            elif seq_env == \"real_road\":\n",
    "                is_drowsy = False\n",
    "                if max_consecutive >= 5 or closed_ratio >= 0.3:\n",
    "                    is_drowsy = True\n",
    "                elif max_consecutive < 3 and closed_ratio < 0.15:\n",
    "                    is_drowsy = False\n",
    "\n",
    "            if path_indicates_normal and not path_indicates_drowsy:\n",
    "                is_drowsy = False\n",
    "\n",
    "            label = 1 if is_drowsy else 0\n",
    "\n",
    "\n",
    "            # # ✅ 환경별 기준 라벨링 - 파일명 무시하고 눈 상태 기반 판단만 사용\n",
    "            # if seq_env == \"controlled\":\n",
    "            #     is_drowsy = (max_consecutive >= 8 or closed_ratio >= 0.35)\n",
    "            # elif seq_env == \"real_road\":\n",
    "            #     is_drowsy = (max_consecutive >= 8 or closed_ratio >= 0.35)\n",
    "\n",
    "            # label = 1 if is_drowsy else 0\n",
    "\n",
    "\n",
    "            # 디버깅 카운터\n",
    "            if max_consecutive >= 8 or closed_ratio >= 0.35:\n",
    "                label_debug['drowsy_eyes'] += 1\n",
    "            elif path_indicates_drowsy:\n",
    "                label_debug['drowsy_path'] += 1\n",
    "            else:\n",
    "                label_debug['normal'] += 1\n",
    "\n",
    "            # ✅ 환경별 카운터\n",
    "            if label == 1:\n",
    "                label_debug['by_environment'][seq_env]['drowsy'] += 1\n",
    "            else:\n",
    "                label_debug['by_environment'][seq_env]['normal'] += 1\n",
    "\n",
    "            X_seq.append(seq_feats)\n",
    "            y_seq.append(label)\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, \"X_seq.npy\"), X_seq)\n",
    "    np.save(os.path.join(save_dir, \"y_seq.npy\"), y_seq)\n",
    "\n",
    "    print(f\"\\n✅ 저장 완료: {save_dir}\")\n",
    "    print(f\"  🔹 총 시퀀스 수: {len(X_seq)}\")\n",
    "    print(f\"  👁 졸음 시퀀스: {np.sum(y_seq)} ({np.sum(y_seq)/len(y_seq)*100:.1f}%)\")\n",
    "    print(f\"  😀 정상 시퀀스: {len(y_seq) - np.sum(y_seq)} ({(len(y_seq) - np.sum(y_seq))/len(y_seq)*100:.1f}%)\")\n",
    "    print(f\"  🔍 라벨링 상세:\")\n",
    "    print(f\"    - 경로 기반 졸음: {label_debug['drowsy_path']}\")\n",
    "    print(f\"    - 눈 상태 기반 졸음: {label_debug['drowsy_eyes']}\")\n",
    "    print(f\"    - 정상: {label_debug['normal']}\")\n",
    "    print(f\"  📊 환경별 분포:\")\n",
    "    for env, counts in label_debug['by_environment'].items():\n",
    "        total = counts['drowsy'] + counts['normal']\n",
    "        if total > 0:\n",
    "            print(f\"    {env}: 졸음 {counts['drowsy']} ({counts['drowsy']/total*100:.1f}%), 정상 {counts['normal']} ({counts['normal']/total*100:.1f}%)\")\n",
    "\n",
    "    return X_seq, y_seq\n",
    "\n",
    "# ✅ 사용 예시\n",
    "data_paths = {\n",
    "    \"controlled\": \"data/ice_control_dataset/ice_project/Training/[라벨]bbox_통제환경\",\n",
    "    \"real_road\": \"data/ice_control_dataset/ice_project/Training/[라벨]bbox_실제도로환경/2.승용\"\n",
    "}\n",
    "\n",
    "X, y = create_combined_sequences(data_paths, save_dir=\"data/lstm_seq_combined\")\n",
    "\n",
    "# ✅ 시각화 실행\n",
    "visualize_realroad_debug(realroad_debug_values)\n",
    "\n",
    "\n",
    "# 🔥 수정된 특성 스케일링 함수 - 이진 특성은 그대로 두고 연속 특성만 스케일링\n",
    "def improved_feature_scaling(X_train, X_val):\n",
    "    \"\"\"이진 특성과 연속 특성을 분리해서 스케일링\"\"\"\n",
    "    \n",
    "    # 특성 분리: [left_eye_open, right_eye_open, mouth_open, face_area_log, left_eye_ratio, right_eye_ratio, mouth_ratio]\n",
    "    binary_features = [0, 1, 2]  # 눈, 입 열림 상태 (이진) - 스케일링 하지 않음\n",
    "    continuous_features = [3, 4, 5, 6]  # 면적, 비율 (연속) - 스케일링 적용\n",
    "    \n",
    "    print(f\"🔍 특성 분리: 이진 특성 {len(binary_features)}개, 연속 특성 {len(continuous_features)}개\")\n",
    "    \n",
    "    # 복사본 생성\n",
    "    X_train_scaled = X_train.copy().astype(np.float32)\n",
    "    X_val_scaled = X_val.copy().astype(np.float32)\n",
    "    \n",
    "    # 🔥 연속 특성만 스케일링\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 3D -> 2D 변환하여 스케일링 (연속 특성만)\n",
    "    train_continuous = X_train[:, :, continuous_features].reshape(-1, len(continuous_features))\n",
    "    val_continuous = X_val[:, :, continuous_features].reshape(-1, len(continuous_features))\n",
    "    \n",
    "    print(f\"🔍 스케일링 전 연속 특성 통계:\")\n",
    "    print(f\"  훈련 데이터: mean={train_continuous.mean(axis=0)}\")\n",
    "    print(f\"  훈련 데이터: std={train_continuous.std(axis=0)}\")\n",
    "    \n",
    "    # 스케일링 적용\n",
    "    train_continuous_scaled = scaler.fit_transform(train_continuous)\n",
    "    val_continuous_scaled = scaler.transform(val_continuous)\n",
    "    \n",
    "    print(f\"🔍 스케일링 후 연속 특성 통계:\")\n",
    "    print(f\"  훈련 데이터: mean={train_continuous_scaled.mean(axis=0)}\")\n",
    "    print(f\"  훈련 데이터: std={train_continuous_scaled.std(axis=0)}\")\n",
    "    \n",
    "    # 다시 3D로 복원하여 연속 특성 부분만 업데이트\n",
    "    X_train_scaled[:, :, continuous_features] = train_continuous_scaled.reshape(\n",
    "        X_train.shape[0], X_train.shape[1], len(continuous_features)\n",
    "    )\n",
    "    X_val_scaled[:, :, continuous_features] = val_continuous_scaled.reshape(\n",
    "        X_val.shape[0], X_val.shape[1], len(continuous_features)\n",
    "    )\n",
    "    \n",
    "    # 🔥 이진 특성은 원본 그대로 유지 (0 또는 1)\n",
    "    print(f\"🔍 이진 특성 확인 (스케일링 안함):\")\n",
    "    print(f\"  left_eye_open 범위: {X_train_scaled[:,:,0].min():.3f} ~ {X_train_scaled[:,:,0].max():.3f}\")\n",
    "    print(f\"  right_eye_open 범위: {X_train_scaled[:,:,1].min():.3f} ~ {X_train_scaled[:,:,1].max():.3f}\")\n",
    "    print(f\"  mouth_open 범위: {X_train_scaled[:,:,2].min():.3f} ~ {X_train_scaled[:,:,2].max():.3f}\")\n",
    "    \n",
    "    # 스케일러에 메타데이터 추가\n",
    "    scaler.binary_features_ = binary_features\n",
    "    scaler.continuous_features_ = continuous_features\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, scaler\n",
    "\n",
    "# ✅ 데이터 분석 함수\n",
    "def analyze_data_distribution(X, y, title=\"Data Analysis\"):\n",
    "    print(f\"\\n📊 {title}\")\n",
    "    print(f\"Shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"Class {u}: {c} samples ({c/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # 특성별 통계\n",
    "    feature_names = ['left_eye_open', 'right_eye_open', 'mouth_open', 'face_area_log', 'left_eye_ratio', 'right_eye_ratio', 'mouth_ratio']\n",
    "    \n",
    "    print(\"\\n📈 특성별 평균값 (졸음 vs 정상):\")\n",
    "    for i, fname in enumerate(feature_names):\n",
    "        if len(X[y==1]) > 0 and len(X[y==0]) > 0:\n",
    "            drowsy_mean = X[y==1, :, i].mean()\n",
    "            normal_mean = X[y==0, :, i].mean()\n",
    "            print(f\"{fname}: 졸음={drowsy_mean:.3f}, 정상={normal_mean:.3f}, 차이={abs(drowsy_mean-normal_mean):.3f}\")\n",
    "        else:\n",
    "            print(f\"{fname}: 한쪽 클래스가 없어서 비교 불가\")\n",
    "\n",
    "# ✅ 개선된 모델 생성\n",
    "def create_improved_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape, dropout=0.3, recurrent_dropout=0.3),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.3),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 🔥 여러 환경의 데이터 경로 설정\n",
    "    train_data_paths = {\n",
    "        \"controlled\": \"data/ice_control_dataset/ice_project/Training/[라벨]bbox_통제환경\",\n",
    "        \"real_road\": \"data/ice_control_dataset/ice_project/Training/[라벨]bbox_실제도로환경/2.승용\"\n",
    "        \n",
    "    }\n",
    "    \n",
    "    val_data_paths = {\n",
    "        \"controlled\": \"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_통제환경\", \n",
    "        \"real_road\": \"data/ice_control_dataset/ice_project/Validation/[라벨]bbox_실제도로환경/승용\"\n",
    "    }\n",
    "    \n",
    "    # 1. 결합된 시퀀스 생성\n",
    "    print(\"🔄 통제환경 + 실제도로환경 데이터 결합하여 시퀀스 생성...\")\n",
    "    \n",
    "    try:\n",
    "        X_train, y_train = create_combined_sequences(\n",
    "            data_paths=train_data_paths,\n",
    "            save_dir=\"data/lstm_seq_combined/train\"\n",
    "        )\n",
    "        \n",
    "        X_val, y_val = create_combined_sequences(\n",
    "            data_paths=val_data_paths,\n",
    "            save_dir=\"data/lstm_seq_combined/val\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 실제 데이터 로드 실패: {e}\")\n",
    "        print(\"🔄 더미 데이터로 테스트...\")\n",
    "        \n",
    "        # 더미 데이터 생성 (균형 잡힌 클래스)\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        seq_len = 30\n",
    "        n_features = 7\n",
    "        \n",
    "        X_train = np.random.random((n_samples, seq_len, n_features))\n",
    "        # 이진 특성은 0 또는 1로 설정\n",
    "        X_train[:, :, :3] = np.random.randint(0, 2, (n_samples, seq_len, 3))\n",
    "        # 균형 잡힌 라벨 생성 (70% 정상, 30% 졸음)\n",
    "        y_train = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "        \n",
    "        X_val = np.random.random((200, seq_len, n_features))\n",
    "        X_val[:, :, :3] = np.random.randint(0, 2, (200, seq_len, 3))\n",
    "        y_val = np.random.choice([0, 1], 200, p=[0.7, 0.3])\n",
    "    \n",
    "    # 2. 데이터 분석\n",
    "    analyze_data_distribution(X_train, y_train, \"Combined Training Data\")\n",
    "    analyze_data_distribution(X_val, y_val, \"Combined Validation Data\")\n",
    "    \n",
    "    # 3. 클래스 균형 체크\n",
    "    class_ratio = np.sum(y_train) / len(y_train)\n",
    "    print(f\"\\n🔍 전체 클래스 분포:\")\n",
    "    print(f\"  정상(0): {len(y_train) - np.sum(y_train)} ({(1-class_ratio)*100:.1f}%)\")\n",
    "    print(f\"  졸음(1): {np.sum(y_train)} ({class_ratio*100:.1f}%)\")\n",
    "    \n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        print(\"❌ 한쪽 클래스만 존재합니다. 라벨링 기준을 재검토해주세요.\")\n",
    "        exit()\n",
    "    \n",
    "    # 4. 🔥 수정된 특성 스케일링\n",
    "    print(\"\\n🔄 수정된 특성 스케일링...\")\n",
    "    X_train_scaled, X_val_scaled, scaler = improved_feature_scaling(X_train, X_val)\n",
    "    \n",
    "    # 스케일러 저장\n",
    "    joblib.dump(scaler, \"feature_scaler_combined_0604.pkl\")\n",
    "    print(\"✅ 스케일러 저장 완료: feature_scaler_combined_0604.pkl\")\n",
    "\n",
    "    \n",
    "    # 5. 클래스 불균형 처리 (필요시)\n",
    "    if class_ratio < 0.2 or class_ratio > 0.8:\n",
    "        print(\"⚠️ 클래스 불균형 감지. 샘플링 고려...\")\n",
    "        \n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        \n",
    "        # LSTM용 데이터를 2D로 변환\n",
    "        X_train_2d = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "        \n",
    "        # 언더샘플링 또는 오버샘플링 적용\n",
    "        if class_ratio < 0.2:  # 졸음이 너무 적으면 오버샘플링\n",
    "            sampler = SMOTE(sampling_strategy=0.3, random_state=42)\n",
    "        else:  # 졸음이 너무 많으면 언더샘플링\n",
    "            sampler = RandomUnderSampler(sampling_strategy=0.4, random_state=42)\n",
    "        \n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_2d, y_train)\n",
    "        \n",
    "        # 다시 3D로 변환\n",
    "        X_train_scaled = X_train_resampled.reshape(-1, X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "        y_train = y_train_resampled\n",
    "        \n",
    "        print(f\"리샘플링 후: {Counter(y_train)}\")\n",
    "    \n",
    "    # 6. 클래스 가중치 계산\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "    print(f\"✅ 클래스 가중치: {class_weight_dict}\")\n",
    "    \n",
    "    # 7. 모델 생성 및 학습\n",
    "    model = create_improved_model((X_train_scaled.shape[1], X_train_scaled.shape[2]))\n",
    "    model.summary()\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(\"best_model_lstm_combined_0604.h5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 8. 결과 평가\n",
    "    y_pred = (model.predict(X_val_scaled) > 0.5).astype(int)\n",
    "    print(\"\\n📊 분류 보고서:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    print(\"\\n📊 혼동 행렬:\")\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    \n",
    "    # 9. 학습 곡선 시각화\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['precision'], label='Train Precision')\n",
    "    plt.plot(history.history['val_precision'], label='Val Precision')\n",
    "    plt.plot(history.history['recall'], label='Train Recall')\n",
    "    plt.plot(history.history['val_recall'], label='Val Recall')\n",
    "    plt.title('Precision/Recall Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7bdd4-aa83-4bde-8f16-510321a5dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 30 --data data/ice_texi.yaml --weights yolov5s.pt --name ice_texi_mouth --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea9d49-804d-460f-83d2-742fcdd1c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py \\\n",
    "--weights runs/train/ice_eye_mouth20/weights/best.pt \\\n",
    "--source data/all_images_실도환_택시_val \\\n",
    "--img 640 \\\n",
    "--conf-thres 0.65 \\\n",
    "--iou-thres 0.45 \\\n",
    "--device 0 \\\n",
    "--save-txt \\\n",
    "--save-conf \\\n",
    "--project runs/detect \\\n",
    "--name ice_yolo_test20_실도환_택시_val \\\n",
    "--exist-ok\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
