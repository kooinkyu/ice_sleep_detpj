{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca9455-a3c7-4bf8-a09b-1e89d2a40f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a87b0-cebe-4949-8f5f-7e37635fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ice_sleep_detpj/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca983a6c-4f2b-40dd-980a-c513f1734c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f08255-80a8-4e0c-9201-3fa436f28eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q /home/jovyan/ice_sleep_detpj/data/ice_control_dataset/ice_project/Validation/[ì›ì²œ]keypoint_ì¤€í†µì œí™˜ê²½.zip -d /home/jovyan/ice_sleep_detpj/data/ice_control_dataset/ice_project/Validation/[ì›ì²œ]keypoint_ì¤€í†µì œí™˜ê²½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42f169-d1a0-4966-8a40-a6227f305117",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d952d7-97cb-415a-ad60-9eefafb68d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/jovyan/ice_sleep_detpj/data/ice_lstm_seq_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b74968-aa78-469a-945a-3ff0c029c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_bbox(bbox, img_width, img_height):\n",
    "    x_min, y_min, x_max, y_max = map(float, bbox)\n",
    "    x_center = (x_min + x_max) / 2 / img_width\n",
    "    y_center = (y_min + y_max) / 2 / img_height\n",
    "    w = (x_max - x_min) / img_width\n",
    "    h = (y_max - y_min) / img_height\n",
    "    return x_center, y_center, w, h\n",
    "\n",
    "def merge_bboxes(b1, b2):\n",
    "    x_min = min(float(b1[0]), float(b2[0]))\n",
    "    y_min = min(float(b1[1]), float(b2[1]))\n",
    "    x_max = max(float(b1[2]), float(b2[2]))\n",
    "    y_max = max(float(b1[3]), float(b2[3]))\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "def json_to_yolo(json_path, output_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    width = int(data['FileInfo']['Width'])\n",
    "    height = int(data['FileInfo']['Height'])\n",
    "    bboxes = data['ObjectInfo']['BoundingBox']\n",
    "\n",
    "    result_lines = []\n",
    "\n",
    "    # ğŸ‘ï¸ ëˆˆ (ì–‘ìª½ ëˆˆ ê¸°ì¤€ í•©ì³ì„œ)\n",
    "    leye = bboxes.get('Leye')\n",
    "    reye = bboxes.get('Reye')\n",
    "    if leye['isVisible'] and reye['isVisible']:\n",
    "        is_open = leye['Opened'] or reye['Opened']\n",
    "        class_id = 0 if is_open else 1  # 0: eye_open, 1: eye_closed\n",
    "        merged_box = merge_bboxes(leye['Position'], reye['Position'])\n",
    "        coords = convert_bbox(merged_box, width, height)\n",
    "        result_lines.append(f\"{class_id} {' '.join(map(str, coords))}\")\n",
    "\n",
    "    # ğŸ‘„ ì…\n",
    "    mouth = bboxes.get('Mouth')\n",
    "    if mouth['isVisible']:\n",
    "        class_id = 2 if mouth['Opened'] else 3  # 2: mouth_open, 3: mouth_closed\n",
    "        coords = convert_bbox(mouth['Position'], width, height)\n",
    "        result_lines.append(f\"{class_id} {' '.join(map(str, coords))}\")\n",
    "\n",
    "    # ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(json_path))[0]\n",
    "    output_file = os.path.join(output_path, base_name + '.txt')\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(result_lines))\n",
    "\n",
    "# ğŸ“ Jupyter í™˜ê²½ ê¸°ì¤€ ê²½ë¡œ (ìˆ˜ì •ë¨)\n",
    "input_dir = 'data/ice_th/YOLO_dataset/labels/val'        # JSON íŒŒì¼ë“¤\n",
    "output_dir = 'data/ice_th/YOLO_dataset/labels/val_txt'   # YOLO txt ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ë³€í™˜ ì‹¤í–‰\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(input_dir, filename)\n",
    "        json_to_yolo(json_path, output_dir)\n",
    "\n",
    "print(\"âœ… ë³€í™˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16bcf0-b9c9-41c0-921f-7e14de70f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def generate_y_true_from_folders(controlled_root, realroad_root, save_path):\n",
    "    \"\"\"\n",
    "    í†µì œí™˜ê²½ì€ ì¡¸ìŒ(1), ì‹¤ì œ ë„ë¡œ í™˜ê²½ì€ ì •ìƒ(0)ìœ¼ë¡œ ë¼ë²¨ë§.\n",
    "    í´ë”ëª…(ì‚¬ëŒ ë‹¨ìœ„)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¼ë²¨ ìƒì„± í›„ numpy ë°°ì—´ë¡œ ì €ì¥.\n",
    "\n",
    "    Args:\n",
    "        controlled_root (str): í†µì œí™˜ê²½ ë°ì´í„° ë£¨íŠ¸ í´ë” ê²½ë¡œ\n",
    "        realroad_root (str): ì‹¤ì œ ë„ë¡œ í™˜ê²½ ë°ì´í„° ë£¨íŠ¸ í´ë” ê²½ë¡œ\n",
    "        save_path (str): ì €ì¥í•  y_true ê²½ë¡œ (npy íŒŒì¼)\n",
    "\n",
    "    Returns:\n",
    "        y_true (np.ndarray): ë¼ë²¨ ë°°ì—´ (0: ì •ìƒ, 1: ì¡¸ìŒ)\n",
    "        person_ids (List[str]): ì‚¬ëŒ ID ë¦¬ìŠ¤íŠ¸ (í´ë”ëª… ê¸°ì¤€)\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    person_ids = []\n",
    "\n",
    "    # âœ… í†µì œí™˜ê²½: ì¡¸ìŒ = 1\n",
    "    if os.path.exists(controlled_root):\n",
    "        for person_folder in sorted(os.listdir(controlled_root)):\n",
    "            full_path = os.path.join(controlled_root, person_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                person_ids.append(person_folder)\n",
    "                y_true.append(1)\n",
    "\n",
    "    # âœ… ì‹¤ì œ ë„ë¡œ í™˜ê²½: ì •ìƒ = 0\n",
    "    if os.path.exists(realroad_root):\n",
    "        for person_folder in sorted(os.listdir(realroad_root)):\n",
    "            full_path = os.path.join(realroad_root, person_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                person_ids.append(person_folder)\n",
    "                y_true.append(0)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    np.save(save_path, y_true)\n",
    "\n",
    "    print(f\"\\nâœ… y_true ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "    print(f\"   ì´ ì¸ì›: {len(y_true)}ëª… | ì¡¸ìŒ: {np.sum(y_true)}, ì •ìƒ: {len(y_true) - np.sum(y_true)}\")\n",
    "    return y_true, person_ids\n",
    "\n",
    "\n",
    "# âœ… Train ë°ì´í„° ë¼ë²¨ë§\n",
    "controlled_root = \"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\"\n",
    "realroad_root = \"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\"\n",
    "y_true_train, person_ids_train = generate_y_true_from_folders(\n",
    "    controlled_root,\n",
    "    realroad_root,\n",
    "    save_path=\"data/y_true_labeling_train.npy\"\n",
    ")\n",
    "\n",
    "# âœ… Validation ë°ì´í„° ë¼ë²¨ë§\n",
    "controlled_val_root = \"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\"\n",
    "realroad_val_root = \"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\"\n",
    "y_true_val, person_ids_val = generate_y_true_from_folders(\n",
    "    controlled_val_root,\n",
    "    realroad_val_root,\n",
    "    save_path=\"data/y_true_labeling_val.npy\"\n",
    ")\n",
    "\n",
    "# âœ… ì „ì²´ person_idì™€ y_true í•©ì¹˜ê¸° (í†µí•© label_dict ë§Œë“¤ê¸° ì›í•  ê²½ìš°)\n",
    "all_person_ids = person_ids_train + person_ids_val\n",
    "all_y_true = np.concatenate([y_true_train, y_true_val])\n",
    "\n",
    "label_dict = dict(zip(all_person_ids, all_y_true))\n",
    "print(f\"\\nâœ… í†µí•© label_dict ìƒì„± ì™„ë£Œ! ì´ ì¸ì› ìˆ˜: {len(label_dict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e2cef-0d0f-4c90-83e4-0da6c9d1be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# âœ… Feature + person_id ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_feature_with_id(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    bb = data[\"ObjectInfo\"][\"BoundingBox\"]\n",
    "\n",
    "    def safe_area(pos):\n",
    "        try:\n",
    "            return max(0, (float(pos[2]) - float(pos[0])) * (float(pos[3]) - float(pos[1])))\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    left_eye_open = int(bb[\"Leye\"][\"Opened\"])\n",
    "    right_eye_open = int(bb[\"Reye\"][\"Opened\"])\n",
    "    mouth_open = int(bb[\"Mouth\"][\"Opened\"])\n",
    "\n",
    "    face_area = safe_area(bb[\"Face\"][\"Position\"])\n",
    "    left_eye_area = safe_area(bb[\"Leye\"][\"Position\"])\n",
    "    right_eye_area = safe_area(bb[\"Reye\"][\"Position\"])\n",
    "    mouth_area = safe_area(bb[\"Mouth\"][\"Position\"])\n",
    "\n",
    "    if face_area > 0:\n",
    "        left_eye_ratio = left_eye_area / face_area\n",
    "        right_eye_ratio = right_eye_area / face_area\n",
    "        mouth_ratio = mouth_area / face_area\n",
    "        face_area_log = np.log(face_area + 1)\n",
    "    else:\n",
    "        left_eye_ratio = right_eye_ratio = mouth_ratio = face_area_log = 0.0\n",
    "\n",
    "    feature = [\n",
    "        left_eye_open,\n",
    "        right_eye_open,\n",
    "        mouth_open,\n",
    "        face_area_log,\n",
    "        left_eye_ratio,\n",
    "        right_eye_ratio,\n",
    "        mouth_ratio,\n",
    "    ]\n",
    "\n",
    "    person_id = os.path.basename(os.path.dirname(json_path))\n",
    "    return feature, person_id\n",
    "\n",
    "# âœ… ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜\n",
    "def create_sequences_with_id(json_dir, label_dict, save_dir, seq_len=30):\n",
    "    X_seq, y_true_seq = [], []\n",
    "    person_to_features = {}\n",
    "\n",
    "    for root, _, files in os.walk(json_dir):\n",
    "        for jf in sorted(files):\n",
    "            if jf.endswith('.json'):\n",
    "                try:\n",
    "                    fpath = os.path.join(root, jf)\n",
    "                    feat, pid = extract_feature_with_id(fpath)\n",
    "                    person_to_features.setdefault(pid, []).append(feat)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ ì˜¤ë¥˜: {jf} - {e}\")\n",
    "\n",
    "    for pid, features in person_to_features.items():\n",
    "        for i in range(len(features) - seq_len):\n",
    "            seq = features[i:i+seq_len]\n",
    "            X_seq.append(seq)\n",
    "            y_true_seq.append(label_dict.get(pid, 0))\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_true_seq = np.array(y_true_seq)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, \"X_seq.npy\"), X_seq)\n",
    "    np.save(os.path.join(save_dir, \"y_true_seq.npy\"), y_true_seq)\n",
    "\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {save_dir}\")\n",
    "    print(f\"  ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)} | ì¡¸ìŒ ë¹„ìœ¨: {np.mean(y_true_seq):.2f}\")\n",
    "    return X_seq, y_true_seq\n",
    "\n",
    "# âœ… person_idsì™€ y_true ì €ì¥ ë¡œì§\n",
    "train_person_ids, val_person_ids = [], []\n",
    "y_true_train, y_true_val = [], []\n",
    "\n",
    "# ğŸ”¹ Train\n",
    "train_paths = [\n",
    "    (\"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\", 1),\n",
    "    (\"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\", 0)\n",
    "]\n",
    "for folder, label in train_paths:\n",
    "    for person_folder in sorted(os.listdir(folder)):\n",
    "        if os.path.isdir(os.path.join(folder, person_folder)):\n",
    "            train_person_ids.append(person_folder)\n",
    "            y_true_train.append(label)\n",
    "\n",
    "# ğŸ”¹ Validation\n",
    "val_paths = [\n",
    "    (\"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\", 1),\n",
    "    (\"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\", 0)\n",
    "]\n",
    "for folder, label in val_paths:\n",
    "    for person_folder in sorted(os.listdir(folder)):\n",
    "        if os.path.isdir(os.path.join(folder, person_folder)):\n",
    "            val_person_ids.append(person_folder)\n",
    "            y_true_val.append(label)\n",
    "\n",
    "# âœ… ì €ì¥\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "np.save(\"data/y_true_labeling_train.npy\", np.array(y_true_train))\n",
    "np.save(\"data/y_true_labeling_val.npy\", np.array(y_true_val))\n",
    "with open(\"data/person_ids_train.txt\", \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(train_person_ids))\n",
    "with open(\"data/person_ids_val.txt\", \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(val_person_ids))\n",
    "\n",
    "# âœ… label_dict êµ¬ì„± (train + val ì „ì²´ ê¸°ì¤€)\n",
    "all_person_ids = train_person_ids + val_person_ids\n",
    "all_y_true = np.concatenate([y_true_train, y_true_val])\n",
    "label_dict = dict(zip(all_person_ids, all_y_true))\n",
    "print(f\"\\nğŸ“Œ label_dict ì´ ì¸ì›: {len(label_dict)}\")\n",
    "\n",
    "# âœ… ì‹œí€€ìŠ¤ ìƒì„±\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_í†µì œ_ytrue/train\"\n",
    ")\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_í†µì œ_ytrue/val\"\n",
    ")\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_ì‹¤ì œë„ë¡œí™˜ê²½_ytrue/train\"\n",
    ")\n",
    "create_sequences_with_id(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\",\n",
    "    label_dict=label_dict,\n",
    "    save_dir=\"data/lstm_seq_ì‹¤ì œë„ë¡œí™˜ê²½_ytrue/val\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160999c4-7d7c-4b89-9885-14c5634cadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# âœ… ì‹¤ì œ ë„ë¡œ í™˜ê²½ (ìŠ¹ìš©)\n",
    "X_train_s = np.load(\"data/lstm_seq_ì‹¤ì œë„ë¡œí™˜ê²½_ytrue/train/X_seq.npy\")\n",
    "y_train_s = np.load(\"data/lstm_seq_ì‹¤ì œë„ë¡œí™˜ê²½_ytrue/train/y_true_seq.npy\")\n",
    "X_val_s = np.load(\"data/lstm_seq_ì‹¤ì œë„ë¡œí™˜ê²½_ytrue/val/X_seq.npy\")\n",
    "y_val_s = np.load(\"data/lstm_seq_ì‹¤ì œë„ë¡œí™˜ê²½_ytrue/val/y_true_seq.npy\")\n",
    "\n",
    "# âœ… í†µì œ í™˜ê²½\n",
    "X_train_c = np.load(\"data/lstm_seq_í†µì œ_ytrue/train/X_seq.npy\")\n",
    "y_train_c = np.load(\"data/lstm_seq_í†µì œ_ytrue/train/y_true_seq.npy\")\n",
    "X_val_c = np.load(\"data/lstm_seq_í†µì œ_ytrue/val/X_seq.npy\")\n",
    "y_val_c = np.load(\"data/lstm_seq_í†µì œ_ytrue/val/y_true_seq.npy\")\n",
    "\n",
    "# âœ… í†µí•©\n",
    "X_train_combined = np.concatenate([X_train_s, X_train_c], axis=0)\n",
    "y_train_combined = np.concatenate([y_train_s, y_train_c], axis=0)\n",
    "X_val_combined = np.concatenate([X_val_s, X_val_c], axis=0)\n",
    "y_val_combined = np.concatenate([y_val_s, y_val_c], axis=0)\n",
    "\n",
    "# âœ… ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "save_dir = \"data/ice_lstm_seq_combined_ytrue\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# âœ… ì €ì¥\n",
    "np.save(os.path.join(save_dir, \"X_seq.npy\"), X_train_combined)\n",
    "np.save(os.path.join(save_dir, \"y_seq.npy\"), y_train_combined)\n",
    "np.save(os.path.join(save_dir, \"X_val_seq.npy\"), X_val_combined)\n",
    "np.save(os.path.join(save_dir, \"y_val_seq.npy\"), y_val_combined)\n",
    "\n",
    "# âœ… ìš”ì•½ ì¶œë ¥\n",
    "print(\"\\nâœ… y_true ê¸°ë°˜ ì‹œí€€ìŠ¤ í†µí•© ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ”¹ Train: X = {X_train_combined.shape}, y = {y_train_combined.shape} (ì¡¸ìŒ: {np.sum(y_train_combined)}, ì •ìƒ: {len(y_train_combined)-np.sum(y_train_combined)})\")\n",
    "print(f\"ğŸ”¹  Val : X = {X_val_combined.shape}, y = {y_val_combined.shape} (ì¡¸ìŒ: {np.sum(y_val_combined)}, ì •ìƒ: {len(y_val_combined)-np.sum(y_val_combined)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b738db-c2ca-4a4e-b653-54a77aa2d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# âœ… Feature + person_id + path ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_feature_with_id(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    bb = data[\"ObjectInfo\"][\"BoundingBox\"]\n",
    "\n",
    "    def safe_area(pos):\n",
    "        try:\n",
    "            return max(0, (float(pos[2]) - float(pos[0])) * (float(pos[3]) - float(pos[1])) )\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    left_eye_open = int(bb[\"Leye\"][\"Opened\"])\n",
    "    right_eye_open = int(bb[\"Reye\"][\"Opened\"])\n",
    "    mouth_open = int(bb[\"Mouth\"][\"Opened\"])\n",
    "\n",
    "    face_area = safe_area(bb[\"Face\"][\"Position\"])\n",
    "    left_eye_area = safe_area(bb[\"Leye\"][\"Position\"])\n",
    "    right_eye_area = safe_area(bb[\"Reye\"][\"Position\"])\n",
    "    mouth_area = safe_area(bb[\"Mouth\"][\"Position\"])\n",
    "\n",
    "    if face_area > 0:\n",
    "        left_eye_ratio = left_eye_area / face_area\n",
    "        right_eye_ratio = right_eye_area / face_area\n",
    "        mouth_ratio = mouth_area / face_area\n",
    "        face_area_log = np.log(face_area + 1)\n",
    "    else:\n",
    "        left_eye_ratio = right_eye_ratio = mouth_ratio = face_area_log = 0.0\n",
    "\n",
    "    feature = [\n",
    "        left_eye_open,\n",
    "        right_eye_open,\n",
    "        mouth_open,\n",
    "        face_area_log,\n",
    "        left_eye_ratio,\n",
    "        right_eye_ratio,\n",
    "        mouth_ratio,\n",
    "    ]\n",
    "\n",
    "    person_id = os.path.basename(os.path.dirname(json_path))\n",
    "    return feature, person_id, json_path\n",
    "\n",
    "# âœ… ì»¤ìŠ¤í…€ ë¼ë²¨ ê¸°ë°˜ ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜\n",
    "def create_sequences_with_custom_label(json_dir, save_dir, seq_len=30):\n",
    "    X_seq, y_seq = [], []\n",
    "    person_to_features = {}\n",
    "\n",
    "    for root, _, files in os.walk(json_dir):\n",
    "        for jf in sorted(files):\n",
    "            if jf.endswith('.json'):\n",
    "                try:\n",
    "                    fpath = os.path.join(root, jf)\n",
    "                    feat, pid, full_path = extract_feature_with_id(fpath)\n",
    "                    person_to_features.setdefault(pid, []).append((feat, full_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ ì˜¤ë¥˜: {jf} - {e}\")\n",
    "\n",
    "    for pid, feat_path_list in person_to_features.items():\n",
    "        features, paths = zip(*feat_path_list)\n",
    "\n",
    "        for i in range(len(features) - seq_len):\n",
    "            seq_feats = features[i:i+seq_len]\n",
    "            seq_paths = paths[i:i+seq_len]\n",
    "\n",
    "            open_stat = np.array(seq_feats)[:, :2]\n",
    "            both_closed = np.sum((open_stat[:, 0] == 0) & (open_stat[:, 1] == 0))\n",
    "\n",
    "            joined_path_str = \" \".join(seq_paths)\n",
    "            if \"ì¡¸ìŒì¬í˜„\" in joined_path_str or \"í•˜í’ˆ\" in joined_path_str or both_closed >= 3:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            X_seq.append(seq_feats)\n",
    "            y_seq.append(label)\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, \"X_seq.npy\"), X_seq)\n",
    "    np.save(os.path.join(save_dir, \"y_seq.npy\"), y_seq)\n",
    "\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {save_dir}\")\n",
    "    print(f\"  ğŸ”¹ ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)}\")\n",
    "    print(f\"  ğŸ‘ ì¡¸ìŒ ì‹œí€€ìŠ¤ ìˆ˜: {np.sum(y_seq)} | ğŸ˜€ ì •ìƒ ì‹œí€€ìŠ¤ ìˆ˜: {len(y_seq) - np.sum(y_seq)}\")\n",
    "    return X_seq, y_seq\n",
    "\n",
    "# âœ… ì‹œí€€ìŠ¤ ìƒì„±: train + val\n",
    "X_train, y_train = create_sequences_with_custom_label(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\",\n",
    "    save_dir=\"data/lstm_seq_custom/train\"\n",
    ")\n",
    "\n",
    "X_val, y_val = create_sequences_with_custom_label(\n",
    "    json_dir=\"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\",\n",
    "    save_dir=\"data/lstm_seq_custom/val\"\n",
    ")\n",
    "\n",
    "# âœ… í†µí•© ì €ì¥\n",
    "save_dir = \"data/ice_lstm_seq_combined_custom\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, \"X_seq.npy\"), X_train)\n",
    "np.save(os.path.join(save_dir, \"y_seq.npy\"), y_train)\n",
    "np.save(os.path.join(save_dir, \"X_val_seq.npy\"), X_val)\n",
    "np.save(os.path.join(save_dir, \"y_val_seq.npy\"), y_val)\n",
    "\n",
    "print(\"\\nğŸ‰ ì „ì²´ í†µí•© ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ”¹ Train: X = {X_train.shape}, y = {y_train.shape} (ì¡¸ìŒ: {np.sum(y_train)}, ì •ìƒ: {len(y_train)-np.sum(y_train)})\")\n",
    "print(f\"ğŸ”¹  Val : X = {X_val.shape}, y = {y_val.shape} (ì¡¸ìŒ: {np.sum(y_val)}, ì •ìƒ: {len(y_val)-np.sum(y_val)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f9c19-f87b-42dd-9c4c-a7b34b1b44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# âœ… 1. ë°ì´í„° ë¡œë“œ (ê²½ë¡œ ìˆ˜ì •)\n",
    "X_train = np.load(\"data/ice_lstm_seq_combined_custom/X_seq.npy\")\n",
    "y_train = np.load(\"data/ice_lstm_seq_combined_custom/y_seq.npy\")\n",
    "X_val = np.load(\"data/ice_lstm_seq_combined_custom/X_val_seq.npy\")\n",
    "y_val = np.load(\"data/ice_lstm_seq_combined_custom/y_val_seq.npy\")\n",
    "\n",
    "print(f\"\\nâœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n",
    "print(f\"ğŸ”¹ X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"ğŸ”¹ X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"ğŸ“Š Train í´ë˜ìŠ¤ ë¶„í¬: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "print(f\"ğŸ“Š Val   í´ë˜ìŠ¤ ë¶„í¬: {dict(zip(*np.unique(y_val, return_counts=True)))}\")\n",
    "\n",
    "# âœ… 2. Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_2d).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val_2d).reshape(X_val.shape)\n",
    "\n",
    "joblib.dump(scaler, \"feature_scaler_custom.pkl\")\n",
    "print(\"âœ… Feature Scaling ì™„ë£Œ ë° ì €ì¥ë¨: feature_scaler_custom.pkl\")\n",
    "\n",
    "# âœ… 3. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weight_dict}\")\n",
    "\n",
    "# âœ… 4. ëª¨ë¸ ì •ì˜\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# âœ… 5. ì½œë°± ì •ì˜\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\"best_model_lstm_custom.h5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "# âœ… 6. í•™ìŠµ\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# âœ… 7. ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\nğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ”¹ Train Loss: {min(history.history['loss']):.4f}\")\n",
    "print(f\"ğŸ”¹ Val Loss:   {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"ğŸ”¹ Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"ğŸ”¹ Val Precision: {max(history.history['val_precision']):.4f}\")\n",
    "print(f\"ğŸ”¹ Val Recall:    {max(history.history['val_recall']):.4f}\")\n",
    "\n",
    "# âœ… 8. í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['precision'], label='Train Precision')\n",
    "plt.plot(history.history['val_precision'], label='Val Precision')\n",
    "plt.plot(history.history['recall'], label='Train Recall')\n",
    "plt.plot(history.history['val_recall'], label='Val Recall')\n",
    "plt.title('Precision/Recall Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705f92e-569c-47a3-86ae-4c92be0440bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e2734-e205-40dc-91de-000e45949061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ë¡œë“œ ì½”ë“œ - í†µì œí™˜ê²½ + ì‹¤ì œë„ë¡œ í™˜ê²½ ê²°í•© ë²„ì „\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# âœ… real_road ë¼ë²¨ ë””ë²„ê·¸ìš© ë¦¬ìŠ¤íŠ¸\n",
    "realroad_debug_values = []\n",
    "\n",
    "def extract_feature_with_id(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    bb = data['ObjectInfo']['BoundingBox']\n",
    "\n",
    "    leye_open = int(bb['Leye']['Opened']) if 'Opened' in bb['Leye'] else 0\n",
    "    reye_open = int(bb['Reye']['Opened']) if 'Opened' in bb['Reye'] else 0\n",
    "    mouth_open = int(bb['Mouth']['Opened']) if 'Opened' in bb['Mouth'] else 0\n",
    "\n",
    "    def safe_area(pos):\n",
    "        try:\n",
    "            return max(0, (float(pos[2]) - float(pos[0])) * (float(pos[3]) - float(pos[1])))\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    face_area = safe_area(bb['Face']['Position'])\n",
    "    if face_area < 10:\n",
    "        face_area = 1e-6\n",
    "\n",
    "    left_eye_area = safe_area(bb['Leye']['Position'])\n",
    "    right_eye_area = safe_area(bb['Reye']['Position'])\n",
    "    mouth_area = safe_area(bb['Mouth']['Position'])\n",
    "\n",
    "    left_eye_ratio = left_eye_area / face_area\n",
    "    right_eye_ratio = right_eye_area / face_area\n",
    "    mouth_ratio = mouth_area / face_area\n",
    "    face_area_log = np.log(face_area + 1)\n",
    "\n",
    "    features = [\n",
    "        float(leye_open),\n",
    "        float(reye_open),\n",
    "        float(mouth_open),\n",
    "        float(face_area_log),\n",
    "        float(left_eye_ratio),\n",
    "        float(right_eye_ratio),\n",
    "        float(mouth_ratio)\n",
    "    ]\n",
    "\n",
    "    person_id = os.path.basename(json_path).split('_')[0]\n",
    "    return features, person_id, json_path\n",
    "\n",
    "\n",
    "# âœ… ì‹œê°í™” í•¨ìˆ˜\n",
    "def visualize_realroad_debug(values):\n",
    "    max_consecutives = [v['max_consecutive'] for v in values]\n",
    "    closed_ratios = [v['closed_ratio'] for v in values]\n",
    "    labels = [v['label'] for v in values]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([m for m, l in zip(max_consecutives, labels) if l == 1], bins=30, alpha=0.7, label='Drowsy')\n",
    "    plt.hist([m for m, l in zip(max_consecutives, labels) if l == 0], bins=30, alpha=0.7, label='Normal')\n",
    "    plt.title('Max Consecutive Eye Closures (real_road)')\n",
    "    plt.xlabel('max_consecutive')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([r for r, l in zip(closed_ratios, labels) if l == 1], bins=30, alpha=0.7, label='Drowsy')\n",
    "    plt.hist([r for r, l in zip(closed_ratios, labels) if l == 0], bins=30, alpha=0.7, label='Normal')\n",
    "    plt.title('Closed Ratio (real_road)')\n",
    "    plt.xlabel('closed_ratio')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_combined_sequences(data_paths, save_dir, seq_len=16):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ í™˜ê²½ì˜ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    data_paths: dictí˜•íƒœë¡œ {'environment_name': 'path'} \n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    person_to_features = {}\n",
    "\n",
    "    # ë””ë²„ê¹…ì„ ìœ„í•œ ì¹´ìš´í„°\n",
    "    label_debug = {\n",
    "        'drowsy_path': 0,\n",
    "        'drowsy_eyes': 0,\n",
    "        'normal': 0,\n",
    "        'by_environment': {}\n",
    "    }\n",
    "\n",
    "    print(f\"ğŸ”„ ë‹¤ìŒ í™˜ê²½ë“¤ì˜ ë°ì´í„°ë¥¼ ê²°í•©í•©ë‹ˆë‹¤:\")\n",
    "    for env_name, env_path in data_paths.items():\n",
    "        print(f\"  ğŸ“ {env_name}: {env_path}\")\n",
    "        label_debug['by_environment'][env_name] = {'drowsy': 0, 'normal': 0}\n",
    "\n",
    "    # ê° í™˜ê²½ë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘\n",
    "    for env_name, json_dir in data_paths.items():\n",
    "        print(f\"\\nğŸ” {env_name} í™˜ê²½ ì²˜ë¦¬ ì¤‘...\")\n",
    "        env_person_features = {}\n",
    "\n",
    "        if not os.path.exists(json_dir):\n",
    "            print(f\"âš ï¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {json_dir}\")\n",
    "            continue\n",
    "\n",
    "        file_count = 0\n",
    "        for root, _, files in os.walk(json_dir):\n",
    "            for jf in sorted(files):\n",
    "                if jf.endswith('.json'):\n",
    "                    try:\n",
    "                        fpath = os.path.join(root, jf)\n",
    "                        feat, pid, full_path = extract_feature_with_id(fpath)\n",
    "                        if feat is not None:\n",
    "                            combined_pid = f\"{env_name}_{pid}\"\n",
    "                            env_person_features.setdefault(combined_pid, []).append((feat, full_path, env_name))\n",
    "                            file_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ ì˜¤ë¥˜: {jf} - {e}\")\n",
    "\n",
    "        print(f\"  âœ… {file_count}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "        for pid, feat_list in env_person_features.items():\n",
    "            person_to_features[pid] = feat_list\n",
    "\n",
    "    print(f\"\\nğŸ“Š ì „ì²´ ìˆ˜ì§‘ëœ ì‚¬ëŒ ìˆ˜: {len(person_to_features)}\")\n",
    "\n",
    "    # ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    for pid, feat_path_env_list in person_to_features.items():\n",
    "        features, paths, envs = zip(*feat_path_env_list)\n",
    "\n",
    "        if len(features) < seq_len:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(features) - seq_len):\n",
    "            seq_feats = features[i:i+seq_len]\n",
    "            seq_paths = paths[i:i+seq_len]\n",
    "            seq_env = envs[i]  # ì²« ë²ˆì§¸ í”„ë ˆì„ì˜ í™˜ê²½\n",
    "\n",
    "            # âœ… ìœ íš¨í•œ í™˜ê²½ì¸ì§€ í™•ì¸\n",
    "            assert seq_env in label_debug['by_environment'], f\"âŒ Unknown environment type: {seq_env}\"\n",
    "\n",
    "            open_stat = np.array(seq_feats)[:, :2]\n",
    "            both_closed = (open_stat[:, 0] == 0) & (open_stat[:, 1] == 0)\n",
    "\n",
    "            consecutive_closed = 0\n",
    "            max_consecutive = 0\n",
    "            for closed in both_closed:\n",
    "                if closed:\n",
    "                    consecutive_closed += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive_closed)\n",
    "                else:\n",
    "                    consecutive_closed = 0\n",
    "\n",
    "            closed_ratio = np.sum(both_closed) / len(both_closed)\n",
    "\n",
    "            joined_path_str = \" \".join(seq_paths)\n",
    "            drowsy_keywords = [\"ì¡¸ìŒì¬í˜„\", \"í•˜í’ˆ\", \"ì¡¸ìŒ\", \"drowsy\", \"yawn\", \"sleepy\", \"tired\"]\n",
    "            normal_keywords = [\"ì •ìƒ\", \"normal\", \"alert\", \"awake\"]\n",
    "\n",
    "            path_indicates_drowsy = any(keyword in joined_path_str.lower() for keyword in drowsy_keywords)\n",
    "            path_indicates_normal = any(keyword in joined_path_str.lower() for keyword in normal_keywords)\n",
    "            \n",
    "\n",
    "\n",
    "            if seq_env == \"controlled\":\n",
    "                is_drowsy = (max_consecutive >= 4 or closed_ratio >= 0.35)\n",
    "                if not is_drowsy and path_indicates_drowsy:\n",
    "                    is_drowsy = True\n",
    "                if max_consecutive < 2 and closed_ratio < 0.15: #ì´ê±° manx_consecutive 2ë¡œ ë°”ê¿”ì„œ í•˜ ìƒê°.\n",
    "                    is_drowsy = False\n",
    "            elif seq_env == \"real_road\":\n",
    "                is_drowsy = False\n",
    "                if max_consecutive >= 5 or closed_ratio >= 0.3:\n",
    "                    is_drowsy = True\n",
    "                elif max_consecutive < 3 and closed_ratio < 0.15:\n",
    "                    is_drowsy = False\n",
    "\n",
    "            if path_indicates_normal and not path_indicates_drowsy:\n",
    "                is_drowsy = False\n",
    "\n",
    "            label = 1 if is_drowsy else 0\n",
    "\n",
    "\n",
    "            # # âœ… í™˜ê²½ë³„ ê¸°ì¤€ ë¼ë²¨ë§ - íŒŒì¼ëª… ë¬´ì‹œí•˜ê³  ëˆˆ ìƒíƒœ ê¸°ë°˜ íŒë‹¨ë§Œ ì‚¬ìš©\n",
    "            # if seq_env == \"controlled\":\n",
    "            #     is_drowsy = (max_consecutive >= 8 or closed_ratio >= 0.35)\n",
    "            # elif seq_env == \"real_road\":\n",
    "            #     is_drowsy = (max_consecutive >= 8 or closed_ratio >= 0.35)\n",
    "\n",
    "            # label = 1 if is_drowsy else 0\n",
    "\n",
    "\n",
    "            # ë””ë²„ê¹… ì¹´ìš´í„°\n",
    "            if max_consecutive >= 8 or closed_ratio >= 0.35:\n",
    "                label_debug['drowsy_eyes'] += 1\n",
    "            elif path_indicates_drowsy:\n",
    "                label_debug['drowsy_path'] += 1\n",
    "            else:\n",
    "                label_debug['normal'] += 1\n",
    "\n",
    "            # âœ… í™˜ê²½ë³„ ì¹´ìš´í„°\n",
    "            if label == 1:\n",
    "                label_debug['by_environment'][seq_env]['drowsy'] += 1\n",
    "            else:\n",
    "                label_debug['by_environment'][seq_env]['normal'] += 1\n",
    "\n",
    "            X_seq.append(seq_feats)\n",
    "            y_seq.append(label)\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, \"X_seq.npy\"), X_seq)\n",
    "    np.save(os.path.join(save_dir, \"y_seq.npy\"), y_seq)\n",
    "\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {save_dir}\")\n",
    "    print(f\"  ğŸ”¹ ì´ ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)}\")\n",
    "    print(f\"  ğŸ‘ ì¡¸ìŒ ì‹œí€€ìŠ¤: {np.sum(y_seq)} ({np.sum(y_seq)/len(y_seq)*100:.1f}%)\")\n",
    "    print(f\"  ğŸ˜€ ì •ìƒ ì‹œí€€ìŠ¤: {len(y_seq) - np.sum(y_seq)} ({(len(y_seq) - np.sum(y_seq))/len(y_seq)*100:.1f}%)\")\n",
    "    print(f\"  ğŸ” ë¼ë²¨ë§ ìƒì„¸:\")\n",
    "    print(f\"    - ê²½ë¡œ ê¸°ë°˜ ì¡¸ìŒ: {label_debug['drowsy_path']}\")\n",
    "    print(f\"    - ëˆˆ ìƒíƒœ ê¸°ë°˜ ì¡¸ìŒ: {label_debug['drowsy_eyes']}\")\n",
    "    print(f\"    - ì •ìƒ: {label_debug['normal']}\")\n",
    "    print(f\"  ğŸ“Š í™˜ê²½ë³„ ë¶„í¬:\")\n",
    "    for env, counts in label_debug['by_environment'].items():\n",
    "        total = counts['drowsy'] + counts['normal']\n",
    "        if total > 0:\n",
    "            print(f\"    {env}: ì¡¸ìŒ {counts['drowsy']} ({counts['drowsy']/total*100:.1f}%), ì •ìƒ {counts['normal']} ({counts['normal']/total*100:.1f}%)\")\n",
    "\n",
    "    return X_seq, y_seq\n",
    "\n",
    "# âœ… ì‚¬ìš© ì˜ˆì‹œ\n",
    "data_paths = {\n",
    "    \"controlled\": \"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\",\n",
    "    \"real_road\": \"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\"\n",
    "}\n",
    "\n",
    "X, y = create_combined_sequences(data_paths, save_dir=\"data/lstm_seq_combined\")\n",
    "\n",
    "# âœ… ì‹œê°í™” ì‹¤í–‰\n",
    "visualize_realroad_debug(realroad_debug_values)\n",
    "\n",
    "\n",
    "# ğŸ”¥ ìˆ˜ì •ëœ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜ - ì´ì§„ íŠ¹ì„±ì€ ê·¸ëŒ€ë¡œ ë‘ê³  ì—°ì† íŠ¹ì„±ë§Œ ìŠ¤ì¼€ì¼ë§\n",
    "def improved_feature_scaling(X_train, X_val):\n",
    "    \"\"\"ì´ì§„ íŠ¹ì„±ê³¼ ì—°ì† íŠ¹ì„±ì„ ë¶„ë¦¬í•´ì„œ ìŠ¤ì¼€ì¼ë§\"\"\"\n",
    "    \n",
    "    # íŠ¹ì„± ë¶„ë¦¬: [left_eye_open, right_eye_open, mouth_open, face_area_log, left_eye_ratio, right_eye_ratio, mouth_ratio]\n",
    "    binary_features = [0, 1, 2]  # ëˆˆ, ì… ì—´ë¦¼ ìƒíƒœ (ì´ì§„) - ìŠ¤ì¼€ì¼ë§ í•˜ì§€ ì•ŠìŒ\n",
    "    continuous_features = [3, 4, 5, 6]  # ë©´ì , ë¹„ìœ¨ (ì—°ì†) - ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
    "    \n",
    "    print(f\"ğŸ” íŠ¹ì„± ë¶„ë¦¬: ì´ì§„ íŠ¹ì„± {len(binary_features)}ê°œ, ì—°ì† íŠ¹ì„± {len(continuous_features)}ê°œ\")\n",
    "    \n",
    "    # ë³µì‚¬ë³¸ ìƒì„±\n",
    "    X_train_scaled = X_train.copy().astype(np.float32)\n",
    "    X_val_scaled = X_val.copy().astype(np.float32)\n",
    "    \n",
    "    # ğŸ”¥ ì—°ì† íŠ¹ì„±ë§Œ ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 3D -> 2D ë³€í™˜í•˜ì—¬ ìŠ¤ì¼€ì¼ë§ (ì—°ì† íŠ¹ì„±ë§Œ)\n",
    "    train_continuous = X_train[:, :, continuous_features].reshape(-1, len(continuous_features))\n",
    "    val_continuous = X_val[:, :, continuous_features].reshape(-1, len(continuous_features))\n",
    "    \n",
    "    print(f\"ğŸ” ìŠ¤ì¼€ì¼ë§ ì „ ì—°ì† íŠ¹ì„± í†µê³„:\")\n",
    "    print(f\"  í›ˆë ¨ ë°ì´í„°: mean={train_continuous.mean(axis=0)}\")\n",
    "    print(f\"  í›ˆë ¨ ë°ì´í„°: std={train_continuous.std(axis=0)}\")\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
    "    train_continuous_scaled = scaler.fit_transform(train_continuous)\n",
    "    val_continuous_scaled = scaler.transform(val_continuous)\n",
    "    \n",
    "    print(f\"ğŸ” ìŠ¤ì¼€ì¼ë§ í›„ ì—°ì† íŠ¹ì„± í†µê³„:\")\n",
    "    print(f\"  í›ˆë ¨ ë°ì´í„°: mean={train_continuous_scaled.mean(axis=0)}\")\n",
    "    print(f\"  í›ˆë ¨ ë°ì´í„°: std={train_continuous_scaled.std(axis=0)}\")\n",
    "    \n",
    "    # ë‹¤ì‹œ 3Dë¡œ ë³µì›í•˜ì—¬ ì—°ì† íŠ¹ì„± ë¶€ë¶„ë§Œ ì—…ë°ì´íŠ¸\n",
    "    X_train_scaled[:, :, continuous_features] = train_continuous_scaled.reshape(\n",
    "        X_train.shape[0], X_train.shape[1], len(continuous_features)\n",
    "    )\n",
    "    X_val_scaled[:, :, continuous_features] = val_continuous_scaled.reshape(\n",
    "        X_val.shape[0], X_val.shape[1], len(continuous_features)\n",
    "    )\n",
    "    \n",
    "    # ğŸ”¥ ì´ì§„ íŠ¹ì„±ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€ (0 ë˜ëŠ” 1)\n",
    "    print(f\"ğŸ” ì´ì§„ íŠ¹ì„± í™•ì¸ (ìŠ¤ì¼€ì¼ë§ ì•ˆí•¨):\")\n",
    "    print(f\"  left_eye_open ë²”ìœ„: {X_train_scaled[:,:,0].min():.3f} ~ {X_train_scaled[:,:,0].max():.3f}\")\n",
    "    print(f\"  right_eye_open ë²”ìœ„: {X_train_scaled[:,:,1].min():.3f} ~ {X_train_scaled[:,:,1].max():.3f}\")\n",
    "    print(f\"  mouth_open ë²”ìœ„: {X_train_scaled[:,:,2].min():.3f} ~ {X_train_scaled[:,:,2].max():.3f}\")\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ëŸ¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "    scaler.binary_features_ = binary_features\n",
    "    scaler.continuous_features_ = continuous_features\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, scaler\n",
    "\n",
    "# âœ… ë°ì´í„° ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_data_distribution(X, y, title=\"Data Analysis\"):\n",
    "    print(f\"\\nğŸ“Š {title}\")\n",
    "    print(f\"Shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"Class {u}: {c} samples ({c/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # íŠ¹ì„±ë³„ í†µê³„\n",
    "    feature_names = ['left_eye_open', 'right_eye_open', 'mouth_open', 'face_area_log', 'left_eye_ratio', 'right_eye_ratio', 'mouth_ratio']\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ íŠ¹ì„±ë³„ í‰ê· ê°’ (ì¡¸ìŒ vs ì •ìƒ):\")\n",
    "    for i, fname in enumerate(feature_names):\n",
    "        if len(X[y==1]) > 0 and len(X[y==0]) > 0:\n",
    "            drowsy_mean = X[y==1, :, i].mean()\n",
    "            normal_mean = X[y==0, :, i].mean()\n",
    "            print(f\"{fname}: ì¡¸ìŒ={drowsy_mean:.3f}, ì •ìƒ={normal_mean:.3f}, ì°¨ì´={abs(drowsy_mean-normal_mean):.3f}\")\n",
    "        else:\n",
    "            print(f\"{fname}: í•œìª½ í´ë˜ìŠ¤ê°€ ì—†ì–´ì„œ ë¹„êµ ë¶ˆê°€\")\n",
    "\n",
    "# âœ… ê°œì„ ëœ ëª¨ë¸ ìƒì„±\n",
    "def create_improved_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape, dropout=0.3, recurrent_dropout=0.3),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.3),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# âœ… ë©”ì¸ ì‹¤í–‰ ì½”ë“œ\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ğŸ”¥ ì—¬ëŸ¬ í™˜ê²½ì˜ ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "    train_data_paths = {\n",
    "        \"controlled\": \"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\",\n",
    "        \"real_road\": \"data/ice_control_dataset/ice_project/Training/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/2.ìŠ¹ìš©\"\n",
    "        \n",
    "    }\n",
    "    \n",
    "    val_data_paths = {\n",
    "        \"controlled\": \"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_í†µì œí™˜ê²½\", \n",
    "        \"real_road\": \"data/ice_control_dataset/ice_project/Validation/[ë¼ë²¨]bbox_ì‹¤ì œë„ë¡œí™˜ê²½/ìŠ¹ìš©\"\n",
    "    }\n",
    "    \n",
    "    # 1. ê²°í•©ëœ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    print(\"ğŸ”„ í†µì œí™˜ê²½ + ì‹¤ì œë„ë¡œí™˜ê²½ ë°ì´í„° ê²°í•©í•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„±...\")\n",
    "    \n",
    "    try:\n",
    "        X_train, y_train = create_combined_sequences(\n",
    "            data_paths=train_data_paths,\n",
    "            save_dir=\"data/lstm_seq_combined/train\"\n",
    "        )\n",
    "        \n",
    "        X_val, y_val = create_combined_sequences(\n",
    "            data_paths=val_data_paths,\n",
    "            save_dir=\"data/lstm_seq_combined/val\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ğŸ”„ ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸...\")\n",
    "        \n",
    "        # ë”ë¯¸ ë°ì´í„° ìƒì„± (ê· í˜• ì¡íŒ í´ë˜ìŠ¤)\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        seq_len = 30\n",
    "        n_features = 7\n",
    "        \n",
    "        X_train = np.random.random((n_samples, seq_len, n_features))\n",
    "        # ì´ì§„ íŠ¹ì„±ì€ 0 ë˜ëŠ” 1ë¡œ ì„¤ì •\n",
    "        X_train[:, :, :3] = np.random.randint(0, 2, (n_samples, seq_len, 3))\n",
    "        # ê· í˜• ì¡íŒ ë¼ë²¨ ìƒì„± (70% ì •ìƒ, 30% ì¡¸ìŒ)\n",
    "        y_train = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "        \n",
    "        X_val = np.random.random((200, seq_len, n_features))\n",
    "        X_val[:, :, :3] = np.random.randint(0, 2, (200, seq_len, 3))\n",
    "        y_val = np.random.choice([0, 1], 200, p=[0.7, 0.3])\n",
    "    \n",
    "    # 2. ë°ì´í„° ë¶„ì„\n",
    "    analyze_data_distribution(X_train, y_train, \"Combined Training Data\")\n",
    "    analyze_data_distribution(X_val, y_val, \"Combined Validation Data\")\n",
    "    \n",
    "    # 3. í´ë˜ìŠ¤ ê· í˜• ì²´í¬\n",
    "    class_ratio = np.sum(y_train) / len(y_train)\n",
    "    print(f\"\\nğŸ” ì „ì²´ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "    print(f\"  ì •ìƒ(0): {len(y_train) - np.sum(y_train)} ({(1-class_ratio)*100:.1f}%)\")\n",
    "    print(f\"  ì¡¸ìŒ(1): {np.sum(y_train)} ({class_ratio*100:.1f}%)\")\n",
    "    \n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        print(\"âŒ í•œìª½ í´ë˜ìŠ¤ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤. ë¼ë²¨ë§ ê¸°ì¤€ì„ ì¬ê²€í† í•´ì£¼ì„¸ìš”.\")\n",
    "        exit()\n",
    "    \n",
    "    # 4. ğŸ”¥ ìˆ˜ì •ëœ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§\n",
    "    print(\"\\nğŸ”„ ìˆ˜ì •ëœ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§...\")\n",
    "    X_train_scaled, X_val_scaled, scaler = improved_feature_scaling(X_train, X_val)\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "    joblib.dump(scaler, \"feature_scaler_combined_0604.pkl\")\n",
    "    print(\"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: feature_scaler_combined_0604.pkl\")\n",
    "\n",
    "    \n",
    "    # 5. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (í•„ìš”ì‹œ)\n",
    "    if class_ratio < 0.2 or class_ratio > 0.8:\n",
    "        print(\"âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°ì§€. ìƒ˜í”Œë§ ê³ ë ¤...\")\n",
    "        \n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        \n",
    "        # LSTMìš© ë°ì´í„°ë¥¼ 2Dë¡œ ë³€í™˜\n",
    "        X_train_2d = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "        \n",
    "        # ì–¸ë”ìƒ˜í”Œë§ ë˜ëŠ” ì˜¤ë²„ìƒ˜í”Œë§ ì ìš©\n",
    "        if class_ratio < 0.2:  # ì¡¸ìŒì´ ë„ˆë¬´ ì ìœ¼ë©´ ì˜¤ë²„ìƒ˜í”Œë§\n",
    "            sampler = SMOTE(sampling_strategy=0.3, random_state=42)\n",
    "        else:  # ì¡¸ìŒì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì–¸ë”ìƒ˜í”Œë§\n",
    "            sampler = RandomUnderSampler(sampling_strategy=0.4, random_state=42)\n",
    "        \n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_2d, y_train)\n",
    "        \n",
    "        # ë‹¤ì‹œ 3Dë¡œ ë³€í™˜\n",
    "        X_train_scaled = X_train_resampled.reshape(-1, X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "        y_train = y_train_resampled\n",
    "        \n",
    "        print(f\"ë¦¬ìƒ˜í”Œë§ í›„: {Counter(y_train)}\")\n",
    "    \n",
    "    # 6. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "    print(f\"âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weight_dict}\")\n",
    "    \n",
    "    # 7. ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = create_improved_model((X_train_scaled.shape[1], X_train_scaled.shape[2]))\n",
    "    model.summary()\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(\"best_model_lstm_combined_0604.h5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 8. ê²°ê³¼ í‰ê°€\n",
    "    y_pred = (model.predict(X_val_scaled) > 0.5).astype(int)\n",
    "    print(\"\\nğŸ“Š ë¶„ë¥˜ ë³´ê³ ì„œ:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    print(\"\\nğŸ“Š í˜¼ë™ í–‰ë ¬:\")\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    \n",
    "    # 9. í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['precision'], label='Train Precision')\n",
    "    plt.plot(history.history['val_precision'], label='Val Precision')\n",
    "    plt.plot(history.history['recall'], label='Train Recall')\n",
    "    plt.plot(history.history['val_recall'], label='Val Recall')\n",
    "    plt.title('Precision/Recall Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7bdd4-aa83-4bde-8f16-510321a5dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 30 --data data/ice_texi.yaml --weights yolov5s.pt --name ice_texi_mouth --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea9d49-804d-460f-83d2-742fcdd1c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py \\\n",
    "--weights runs/train/ice_eye_mouth20/weights/best.pt \\\n",
    "--source data/all_images_ì‹¤ë„í™˜_íƒì‹œ_val \\\n",
    "--img 640 \\\n",
    "--conf-thres 0.65 \\\n",
    "--iou-thres 0.45 \\\n",
    "--device 0 \\\n",
    "--save-txt \\\n",
    "--save-conf \\\n",
    "--project runs/detect \\\n",
    "--name ice_yolo_test20_ì‹¤ë„í™˜_íƒì‹œ_val \\\n",
    "--exist-ok\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
